{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerías que serán utilizadas para esta notebook\n",
    "import numpy as np\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "plt.rcParams['figure.figsize'] = [10,10]\n",
    "\n",
    "from propagacion_inversa import X,Y,Z\n",
    "import propagacion_inversa as ut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que tenemos una función sencilla.\n",
    "\\begin{align}\n",
    "f(x,y) = x^2y^2 \\\\\n",
    "\\end{align}\n",
    "\n",
    "Y calculamos las derivadas parciales con respecto a su variables. De esto, obtenemos dos funciones nuevas.\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{matrix}\n",
    "\\frac{\\delta f}{\\delta x} = 2y & &\n",
    "\\frac{\\delta f}{\\delta y} = 2x\n",
    "\\end{matrix}\n",
    "\\end{align}\n",
    "\n",
    "¿Qué quiere decir esto?\n",
    "\n",
    "Recordando, para conocer la pendiente de una recta se debe calcular su razón de cambio, en otras palabras, la división entre las diferencia en los ejes de dos puntos. Vease la figura siguiente. \n",
    "\n",
    "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vQcbvKqXTmsWpEsuoEbx4XRPofEMbHYsa1N2gsH0chzwWUuGbuqcwoxmWokPKPvVAqUUFmJIoreo2Um/pub?w=506&h=477\">\n",
    "\n",
    "Entonces ¿qué conocimiento aporta el conocer su derivada?. Es un indicador de la dirección en la cual crece la función. En el caso de la recta en la figura, su derivada es positiva, ya que de derecha a izquierda el valor en $y$ crece.\n",
    "\n",
    "\n",
    "Volviendo al ejemplo original. Supongamos que queremos encontrar las coordenadas en $x$ y $y$ que produzcan el valor $z$ más bajo posible, y que solo contamos con $N$ número de intentos. Una aproximación podría ser el de intentar adivinar o seleccionar de manera aleatorea las coordenadas $x,y$, o bien, utilizar el gradiente de la función para determinar de que manera debemos modificar nuestros valores en $x,y$ para cumplir nuestro objetivo.\n",
    "\n",
    "Imaginemos que decidimos iniciar la busqueda del punto más bajo en \\[4,4\\]. Dado que $f(x,y)=xy$ sabemos que $f(4,4)=256$.\n",
    "\n",
    "A partir del gradiente de $f(x,y)$ ,que determinamos anteriormente, sabemos que la forma en la que la variable $x$ contribuye a $z$ es $2y$. De igual manera, la forma en la que la variable $y$ contribuye a $z$ es $2x$. Si remplazamos las variables por números, se obtiene que $\\frac{\\delta f}{\\delta x} = 8$ y $\\frac{\\delta f}{\\delta y} = 8$. Interpretado de otra manera, esto indica que al incrementar $x$ o $y$, $z$ incrementará debido a que las derivadas parciales tanto de $x$ como de $y$ fueron positivas, por lo que si deseamos reducir el valor de $z$ debemos movernos al sentido opuesto.\n",
    "\n",
    "Para nuestra siguiente oportunidad, decidimos seleccionar el punto \\[3.5,3\\]. Con la prueba anterior comprobamos que si decrementamos ambos valores, el valor de z se reducirá. Lo cual es correcto, $f(3.5,3) = 110.25$. Pasamos de tener un valor de 256 en $z$ a 110.25. Si volvemos a calcular el gradiente, obtenemos que $\\frac{\\delta f}{\\delta x} = 3$ y $\\frac{\\delta f}{\\delta y} = 3.5$, por lo que volvemos a disminuir el valor de $x$ y $y$.\n",
    "\n",
    "Así en varias ocaciones:\n",
    "- $x=3$, $y=2.5$ --> $z=4$.   $\\frac{\\delta f}{\\delta x} = 2.5$ y $\\frac{\\delta f}{\\delta y} = 3$ DISMINUIMOS $x$ Y $y$.\n",
    "\n",
    "- $x=2.5$, $y=2$ --> $z=0.25$. $\\frac{\\delta f}{\\delta x} = 2$ y $\\frac{\\delta f}{\\delta y} = 2.5$ DECIDIMOS DISMINUIR SOLO $y$.\n",
    "\n",
    "- $x=2$, $y=1$ --> $z=0.04$. $\\frac{\\delta f}{\\delta x} = 1$ y $\\frac{\\delta f}{\\delta y} = 2$ TODO BIEN. DECIDIMOS DISMINUIR SOLO $y$.\n",
    "\n",
    "- $x=1$, $y=0.5$ --> $z=0.01$. $\\frac{\\delta f}{\\delta x} = 0.5$ y $\\frac{\\delta f}{\\delta y} = 1$ TODO BIEN.\n",
    "\n",
    "\n",
    "Como todo parece ir bien con la variación en $y$ y en $x$, decidimos reducimos a $x$ en 4 unidades y a $y$ establecerla como -1. Sin embargo, encontramos un pequeño problema. El valor de $z$ en $(-3,-1)$ es de 9, que es mayor que 0.01. Desesperados por tratar de corregir esto, regresamos al gradiente de nuestra función, que muestra lo siguiente $\\frac{\\delta f}{\\delta x} = -1$ y $\\frac{\\delta f}{\\delta y} = -3$. Esto indica que si deseamos volver a reducir el valor de $z$ devemos aumentar el valor de ambas variables.\n",
    "\n",
    "Y esto se repite $N$ veces hasta.\n",
    "\n",
    "A continuación podemos apreciar vizualmente lo que hicimos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d30d00736d0406fba2480e2edb8ded9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.array([4,3.5,3  ,2.5,2  ,1  ,-3])\n",
    "y = np.array([4,3  ,2.5,2  ,1,0.5,-1])\n",
    "z = ut.fxy(x,y)\n",
    "lbl = ut.crear_lbs(x,y,z)\n",
    "# Para eliminar las etiquetas, descomentar la siguiente linea\n",
    "#lbl = ['']*len(lbl)\n",
    "ut.mostrar_superficie_xy(x,y,z,lbl,pp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
